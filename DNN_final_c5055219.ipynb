{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVBEk5nVjmeTADJwoOy16x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c5055219/final_template/blob/main/DNN_final_c5055219.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XMMftzQo2L7d"
      },
      "outputs": [],
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 1: The data preparation\n",
        "First we need to activate our google drive so that we can save out data permanently.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Upl_LcT4Nxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setting up google drive to save checkpoints\n",
        "\n",
        "# This will prompt you to authorize Google Drive access\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n",
        "    \"\"\"\n",
        "    # 1. Define the full Google Drive path\n",
        "    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "\n",
        "    # Ensure the directory exists before attempting to save\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "    # 2. Combine the folder and the filename\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # 3. Create the checkpoint dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "\n",
        "    # 4. Save the dictionary to the Google Drive path\n",
        "    torch.save(checkpoint, full_path)\n",
        "    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n",
        "\n",
        "\n",
        "def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n",
        "    \"\"\"\n",
        "    # Define the same Google Drive folder path\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # Check if the checkpoint file exists\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n",
        "\n",
        "    # Restore model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Restore optimizer state (if provided)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Extract metadata\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    loss = checkpoint.get('loss', None)\n",
        "\n",
        "    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n",
        "\n",
        "    return model, optimizer, epoch, loss\n"
      ],
      "metadata": {
        "id": "Y_whoNAy3ePj",
        "outputId": "ab52aac4-9799-4601-8b9f-efe7425858d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to load images and process data"
      ],
      "metadata": {
        "id": "2TtIReBx5IXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions to load images and process data\n",
        "\n",
        "\n",
        "# This function just extracts the tags from the text, don't get distracted by it.\n",
        "# I changed this function a bit to fix some bugs\n",
        "def parse_gdi_text(text):\n",
        "    \"\"\"Parse GDI formatted text into structured data\"\"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    images = []\n",
        "\n",
        "    for gdi in soup.find_all('gdi'):\n",
        "        # Debug: print what BeautifulSoup sees\n",
        "\n",
        "\n",
        "        image_id = None\n",
        "        if gdi.attrs:\n",
        "            # Check for attributes like 'image1', 'image2', etc.\n",
        "            for attr_name, attr_value in gdi.attrs.items():\n",
        "                if 'image' in attr_name.lower():\n",
        "                    image_id = attr_name.replace('image', '')\n",
        "                    break\n",
        "\n",
        "\n",
        "        if not image_id:\n",
        "            tag_str = str(gdi)\n",
        "            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n",
        "            if match:\n",
        "                image_id = match.group(1)\n",
        "\n",
        "\n",
        "        if not image_id:\n",
        "            image_id = str(len(images) + 1)\n",
        "\n",
        "        content = gdi.get_text().strip()\n",
        "\n",
        "        # Extract tagged elements using BeautifulSoup directly\n",
        "        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n",
        "        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n",
        "        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n",
        "\n",
        "        images.append({\n",
        "            'image_id': image_id,\n",
        "            'description': content,\n",
        "            'objects': objects,\n",
        "            'actions': actions,\n",
        "            'locations': locations,\n",
        "            'raw_text': str(gdi)\n",
        "        })\n",
        "\n",
        "    return images\n",
        "\n",
        "# This is an utility function to show images.\n",
        "# Why do we need to do all this?\n",
        "def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n",
        "  \"\"\"\n",
        "  De-normalize the image (if necessary) and show image\n",
        "  \"\"\"\n",
        "  if de_normalize:\n",
        "    new_mean = -img_mean/img_std\n",
        "    new_std = 1/img_std\n",
        "\n",
        "    image = transforms.Normalize(\n",
        "        mean=new_mean,\n",
        "        std=new_std\n",
        "    )(image)\n",
        "  ax.imshow(image.permute(1, 2, 0))\n",
        "\n"
      ],
      "metadata": {
        "id": "8-JxmLWN5JoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "plkSyNzE5TIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
      ],
      "metadata": {
        "id": "OAlxjFEm5S2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
      ],
      "metadata": {
        "id": "XjZBWEeq5d3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three datasets\n",
        "We will create three different dataset objects and the corresponding loaders for performing multiple tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "sQjhxrwZ5iPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main dataset\n",
        "class SequencePredictionDataset(Dataset):\n",
        "    def __init__(self, original_dataset, tokenizer):\n",
        "        self.dataset = original_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((64, 64)),\n",
        "          transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      images = self.dataset[idx]['images']\n",
        "      descriptions = parse_gdi_text(self.dataset[idx]['story'])\n",
        "\n",
        "      input_images = []\n",
        "      input_texts = []\n",
        "\n",
        "      for i in range(4):\n",
        "        img = FT.equalize(images[i])\n",
        "        input_images.append(self.transform(img))\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            descriptions[i], padding ='max_length', truncation = True,\n",
        "            max_length=64, return_tensors='pt'\n",
        "        ).input_ids.squeeze(0)\n",
        "        input_texts.append(tokens)\n",
        "\n",
        "\n",
        "      target_image =self.transform(FT.equalilze(images[4]))\n",
        "      target_tokens =self.tokenizer(\n",
        "          descriptions[4],padding+'max_length', truncation=True,\n",
        "          max_length=64, retrun_tensors='pt'\n",
        "      ).input_ids.squeeze(0)\n",
        "\n",
        "\n",
        "      return(\n",
        "          torch.stack(input_images),\n",
        "          torch.stack(input_texts),\n",
        "          target_image,\n",
        "          target_tokens,\n",
        "      )\n",
        "\n"
      ],
      "metadata": {
        "id": "QHTu8O8r5jIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic Multimodal Dataset"
      ],
      "metadata": {
        "id": "XDAGF1nP5rva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalSequenceDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Each sample:\n",
        "    -image sequence: (T, image_dim)\n",
        "    -text sequence: (T, text_dim)\n",
        "    -target: next-step multimodal vector\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "                 num_samples=1000,\n",
        "                 seq_len=5,\n",
        "                 image_dim=128,\n",
        "                 text_dim=64):\n",
        "        self.data = []\n",
        "        for _ in range(num_samples):\n",
        "            images = torch.randn(seq_len, image_dim)\n",
        "            texts = torch.randn(seq_len, text_dim)\n",
        "            target = torch.randn(image_dim + text_dim)\n",
        "            self.data.append((images, texts, target))\n",
        "  def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = MultimodalSequenceDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "qGF2O1Gx5xd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text task dataset (text autoencoding)"
      ],
      "metadata": {
        "id": "vgzL7Yv250yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Text task dataset (text autoencoding)\n",
        "class TextTaskDataset(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "      self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      emb = self.embedding(x)\n",
        "      _, (h, c) = self.lstm(emb)\n",
        "      return h,c\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "class TextDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        emb = self.embedding(x)\n",
        "        out, _ = self.lstm(emb, (h, c))\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "OuIa2IlX50fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset for image autoencoder task"
      ],
      "metadata": {
        "id": "o1NEJEZW57s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset for image autoencoder task\n",
        "class AutoEncoderTaskDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((240, 500)),# Reasonable size based on our previous analysis\n",
        "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      num_frames = self.dataset[idx][\"frame_count\"]\n",
        "      frames = self.dataset[idx][\"images\"]\n",
        "\n",
        "      # Pick a frame at random\n",
        "      frame_idx = torch.randint(0, num_frames-1, (1,)).item()\n",
        "      input_frame = self.transform(frames[frame_idx]) # Input to the autoencoder\n",
        "\n",
        "      return input_frame,"
      ],
      "metadata": {
        "id": "89nKFHG15_O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Creating and testing our dataset objects and loaders\n"
      ],
      "metadata": {
        "id": "CIEqUfql6OO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Modal Attention"
      ],
      "metadata": {
        "id": "F23jXxsM84li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CrossModalAttention(nn.Module):\n",
        "def __init__(self, dim):\n",
        "       super().__init__()\n",
        "       self.query = nn.Linear(dim, dim)\n",
        "       self.key   = nn.Linear(dim, dim)\n",
        "       self.value = nn.Linear(dim, dim)\n",
        "       self.scale = dim ** -0.5\n",
        "\n",
        "def forward(self, text_feats, visual_feats):\n",
        "         \"\"\"\n",
        "         text_feats:  [batch*seq, dim]\n",
        "         visual_feats: [batch*seq, dim]\n",
        "         \"\"\"\n",
        "\n",
        "         Q = self.query(text_feats)      # Queries from text\n",
        "         K = self.key(visual_feats)      # Keys from image\n",
        "         V = self.value(visual_feats)    # Values from image\n",
        "\n",
        "         attn_scores = torch.matmul(Q, K.T) * self.scale\n",
        "         attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "         attended = torch.matmul(attn_weights, V)\n",
        "\n",
        "         fused = attended + text_feats   # Add residual connection\n",
        "         return fused\n"
      ],
      "metadata": {
        "id": "SqiEDsQR6Tq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Cross Modal Attention"
      ],
      "metadata": {
        "id": "imvZTJzy8806"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Image attends to text and text attends to image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "\n",
        "    def forward(self, img_feat, txt_feat):\n",
        "        # (B, T, H)\n",
        "        score_img_txt = torch.matmul(\n",
        "            img_feat, txt_feat.transpose(-2, -1)\n",
        "        ) / self.scale\n",
        "\n",
        "        score_txt_img = torch.matmul(\n",
        "            txt_feat, img_feat.transpose(-2, -1)\n",
        "        ) / self.scale\n",
        "\n",
        "        w_img_txt = F.softmax(score_img_txt, dim=-1)\n",
        "        w_txt_img = F.softmax(score_txt_img, dim=-1)\n",
        "\n",
        "        img_context = torch.matmul(w_img_txt, txt_feat)\n",
        "        txt_context = torch.matmul(w_txt_img, img_feat)\n",
        "\n",
        "        return torch.cat([img_context, txt_context], dim=-1)\n"
      ],
      "metadata": {
        "id": "x3ylVS736Xxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence Model"
      ],
      "metadata": {
        "id": "wiJx9rCv9BWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return out[:, -1, :]"
      ],
      "metadata": {
        "id": "WemssLhq6bVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Sequence prediction task"
      ],
      "metadata": {
        "id": "cZfjOS996epR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the Sequence prediction task\n",
        "class SequencePredictionDataset(Dataset):\n",
        "    def __init__(self, original_dataset, tokenizer):\n",
        "        self.dataset = original_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((64, 64)),\n",
        "          transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      images = self.dataset[idx]['images']\n",
        "      # Parse the full GDI story text once\n",
        "      parsed_story_descriptions = parse_gdi_text(self.dataset[idx]['story'])\n",
        "\n",
        "      input_images = []\n",
        "      input_texts = []\n",
        "\n",
        "      for i in range(4):\n",
        "        img = FT.equalize(images[i])\n",
        "        input_images.append(self.transform(img))\n",
        "\n",
        "        # Use the 'description' field from the parsed GDI output\n",
        "        tokens = self.tokenizer(\n",
        "            parsed_story_descriptions[i]['description'], # Corrected to access the description string\n",
        "            padding ='max_length', truncation = True,\n",
        "            max_length=64, return_tensors='pt'\n",
        "        ).input_ids.squeeze(0)\n",
        "        input_texts.append(tokens)\n",
        "\n",
        "\n",
        "      target_image = self.transform(FT.equalize(images[4])) # Fixed typo: equalilze -> equalize\n",
        "      target_tokens = self.tokenizer(\n",
        "          parsed_story_descriptions[4]['description'], # Corrected to access the description string\n",
        "          padding='max_length', truncation=True, # Fixed typo: padding+'max_length' -> padding='max_length'\n",
        "          max_length=64, return_tensors='pt' # Fixed typo: retrun_tensors -> return_tensors\n",
        "      ).input_ids.squeeze(0)\n",
        "\n",
        "\n",
        "      return(\n",
        "          torch.stack(input_images),\n",
        "          torch.stack(input_texts),\n",
        "          target_image,\n",
        "          target_tokens,\n",
        "      )\n",
        "\n",
        "\n",
        "class SequencePredictor(nn.Module):\n",
        "    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim, gru_hidden_dim):\n",
        "        # The content of the __init__ method was missing, adding a pass for now.\n",
        "        # This method likely needs to be filled with the actual initialization logic\n",
        "        # for the SequencePredictor model's layers and components.\n",
        "        # Based on a later cell (pOQwqPPpk6pP) which defines SequencePredictor, I've filled in the __init__.\n",
        "        super(SequencePredictor, self).__init__()\n",
        "\n",
        "        # --- 1. Static Encoders ---\n",
        "        self.image_encoder = visual_autoencoder.encoder\n",
        "        self.text_encoder = text_autoencoder.encoder\n",
        "\n",
        "        # --- 2. Temporal Encoder ---\n",
        "        fusion_dim = latent_dim * 2 # z_visual + z_text\n",
        "        self.temporal_rnn = nn.GRU(fusion_dim, latent_dim, batch_first=True)\n",
        "\n",
        "        # --- 3. Attention ---\n",
        "        # Assuming Attention is a defined class elsewhere\n",
        "        self.attention = CrossModalAttention(latent_dim) # Using CrossModalAttention as a placeholder for Attention\n",
        "\n",
        "        # --- 4. Final Projection ---\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # --- 5. Decoders ---\n",
        "        self.image_decoder = visual_autoencoder.decoder\n",
        "        self.text_decoder = text_autoencoder.decoder\n",
        "\n",
        "        self.fused_to_h0 = nn.Linear(latent_dim, gru_hidden_dim)\n",
        "        self.fused_to_c0 = nn.Linear(latent_dim, gru_hidden_dim)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset\n",
        "sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer) # Instantiate the test dataset\n",
        "\n",
        "# Let's do things properly, we will also have a validation split\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(sp_train_dataset))\n",
        "val_size = len(sp_train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(sp_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Instantiate the dataloaders\n",
        "train_dataloader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
        "# We will use the validation set to visualize the progress.\n",
        "val_dataloader = DataLoader(val_subset, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "BfeVpL6I6eDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the text task\n",
        "class TextTaskDataset(Dataset):\n",
        "    def __init__(self, original_dataset):\n",
        "        self.texts = []\n",
        "        for item in original_dataset:\n",
        "            parsed_descriptions = parse_gdi_text(item['story'])\n",
        "            for desc_item in parsed_descriptions:\n",
        "                self.texts.append(desc_item['description'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "text_dataset = TextTaskDataset(train_dataset)\n",
        "text_dataloader = DataLoader(text_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "nSLyBD3f61XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaselineMultiModel"
      ],
      "metadata": {
        "id": "STfbhCRG9FyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineMultimodalModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_dim=128,\n",
        "                 text_dim=64,\n",
        "                 hidden_dim=128,\n",
        "                 seq_hidden=128,\n",
        "                 output_dim=192):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_encoder = ImageEncoder(image_dim, hidden_dim)\n",
        "        self.text_encoder = TextEncoder(text_dim, hidden_dim)\n",
        "        self.sequence_model = SequenceModel(hidden_dim * 2, seq_hidden)\n",
        "        self.output_layer = nn.Linear(seq_hidden, output_dim)\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        img_feat = self.image_encoder(images)\n",
        "        txt_feat = self.text_encoder(texts)\n",
        "        fused = torch.cat([img_feat, txt_feat], dim=-1)\n",
        "        seq_out = self.sequence_model(fused)\n",
        "        return self.output_layer(seq_out)"
      ],
      "metadata": {
        "id": "R11JvAjB6-rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the image autoencoder task\n",
        "autoencoder_dataset = AutoEncoderTaskDataset(train_dataset)\n",
        "autoencoder_dataloader = DataLoader(autoencoder_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "M7YS5PUe7Dfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing some of the outputs of the SP dataset\n",
        "frames, descriptions, image_target, text_target = sp_train_dataset[np.random.randint(0,400)]\n",
        "\n",
        "\n",
        "# frames -> [seq_len, channels, height, width]\n",
        "print(\"Description: \", descriptions.shape)\n",
        "print(\"Shape of frames:\", frames.shape)\n",
        "figure, ax = plt.subplots(1,1)\n",
        "show_image(ax, image_target)\n",
        "\n",
        "# Show the 4 images of the input (\"frames\") using the show_images(...)function\n",
        "\n",
        "# Do some tests on the batches (try with batch size small)\n",
        "frames, descriptions, image_target, text_target = next(iter(train_dataloader))\n",
        "print(frames.shape)\n",
        "print(descriptions.shape)"
      ],
      "metadata": {
        "id": "OIxyw1S27Kke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 2: Models\n"
      ],
      "metadata": {
        "id": "pXc1_CsL7PB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The text autoencoder (Seq2Seq)\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "      Encodes a sequence of tokens into a latent space representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "      Decodes a latent space representation into a sequence of tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n",
        "\n",
        "    def forward(self, input_seq, hidden, cell):\n",
        "        # Debug prints removed for brevity once information was gathered\n",
        "        embedded = self.embedding(input_seq)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.out(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "# We create the basic text autoencoder (a special case of a sequence to sequence model)\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, target_seq):\n",
        "        # input_seq and target_seq are both your 'input_ids'\n",
        "        # 1. Encode the input sequence\n",
        "        _enc_out, hidden, cell = self.encoder(input_seq)\n",
        "\n",
        "        # 2. Create the \"shifted\" decoder input for teacher forcing.\n",
        "        # We want to predict target_seq[:, 1:]\n",
        "        # So, we feed in target_seq[:, :-1]\n",
        "        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n",
        "        decoder_input = target_seq[:, :-1]\n",
        "\n",
        "        # 3. Run the decoder *once* on the entire sequence.\n",
        "        # It takes the encoder's final state (hidden, cell)\n",
        "        # and the full \"teacher\" sequence (decoder_input).\n",
        "        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "gpK0jmK87QgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions for NLP tasks\n",
        "def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id):\n",
        "      \"\"\"\n",
        "        This function generates a sequence of tokens using the provided decoder.\n",
        "      \"\"\"\n",
        "      # Ensure the model is in evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # 2. SETUP DECODER INPUT\n",
        "      # Start with the SOS token, shape (1, 1)\n",
        "      dec_input = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n",
        "      # hidden = torch.zeros(1, 1, hidden_dim, device=device)\n",
        "      # cell = torch.zeros(1, 1, hidden_dim, device=device)\n",
        "\n",
        "      generated_tokens = []\n",
        "\n",
        "      # 3. AUTOREGRESSIVE LOOP\n",
        "      for _ in range(max_len):\n",
        "          with torch.no_grad():\n",
        "              # Run the decoder one step at a time\n",
        "              # dec_input is (1, 1) hereâ€”it's just the last predicted token\n",
        "              prediction, hidden, cell = model(dec_input, hidden, cell)\n",
        "\n",
        "          logits = prediction.squeeze(1) # Shape (1, vocab_size)\n",
        "          temperature = 0.9 # <--- Try a value between 0.5 and 1.0\n",
        "\n",
        "          # 1. Divide logits by temperature\n",
        "          # 2. Apply softmax to get probabilities\n",
        "          # 3. Use multinomial to sample one token based on the probabilities\n",
        "          probabilities = torch.softmax(logits / temperature, dim=-1)\n",
        "          next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "          token_id = next_token.squeeze().item()\n",
        "\n",
        "          # Check for the End-of-Sequence token\n",
        "          if token_id == eos_token_id:\n",
        "              break\n",
        "\n",
        "          if token_id == 0 or token_id == sos_token_id:\n",
        "              continue\n",
        "\n",
        "            # Append the predicted token\n",
        "          generated_tokens.append(token_id)\n",
        "\n",
        "          # The predicted token becomes the input for the next iteration\n",
        "          dec_input = next_token\n",
        "\n",
        "      # Return the list of generated token IDs\n",
        "      return generated_tokens\n"
      ],
      "metadata": {
        "id": "IPcBONog7Zgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Do some tests\n",
        "# desc = text_dataset[np.random.randint(0, 100)]\n",
        "# print(f\"Input: {desc}\")\n",
        "# input_ids = tokenizer(desc, return_tensors=\"pt\",  padding=True, truncation=True).input_ids\n",
        "# input_ids = input_ids.to(device)\n",
        "# generated_tokens = generate(model, hidden, cell, max_len=100, sos_token_id=tokenizer.cls_token_id, eos_token_id=tokenizer.sep_token_id)\n",
        "# print(\"Output: \", tokenizer.decode(generated_tokens))"
      ],
      "metadata": {
        "id": "-LsGAZ0R7hpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The visual autoencoder\n",
        "class Backbone(nn.Module):\n",
        "    \"\"\"\n",
        "      Main convolutional blocks for our CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 8):\n",
        "        super(Backbone, self).__init__()\n",
        "        # Encoder convolutional layers\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n",
        "            nn.GroupNorm(8, 16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        # Calculate flattened dimension for linear layer\n",
        "        self.flatten_dim = 64 * output_w * output_h\n",
        "        # Latent space layers\n",
        "        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n",
        "        z = self.fc1(x)\n",
        "        return z\n",
        "class VisualDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "      Decodes a latent representation into a content image and a context image\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 8):\n",
        "        super(VisualDecoder, self).__init__()\n",
        "        self.imh = 64 # Updated to match transforms.Resize((64, 64))\n",
        "        self.imw = 64 # Updated to match transforms.Resize((64, 64))\n",
        "        self.flatten_dim = 64 * output_w * output_h\n",
        "        self.output_w = output_w\n",
        "        self.output_h = output_h\n",
        "\n",
        "        self.fc1 = nn.Linear(latent_dim, self.flatten_dim)\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "          nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # Corrected output_padding\n",
        "          nn.GroupNorm(8, 32),\n",
        "          nn.LeakyReLU(0.1),\n",
        "\n",
        "          nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1), # Corrected output_padding\n",
        "          nn.GroupNorm(8, 16),\n",
        "          nn.LeakyReLU(0.1),\n",
        "\n",
        "          nn.ConvTranspose2d(16, 3, kernel_size=7, stride=2, padding=3, output_padding=1), # Corrected output_padding\n",
        "          nn.Sigmoid() # Use nn.Tanh() if your data is normalized to [-1, 1]\n",
        "      )\n",
        "\n",
        "    def forward(self, z):\n",
        "      x = self.fc1(z)\n",
        "\n",
        "      x_content = self.decode_image(x)\n",
        "      x_context = self.decode_image(x)\n",
        "\n",
        "      return x_content, x_context\n",
        "\n",
        "    def decode_image(self, x):\n",
        "      x = x.view(-1, 64, self.output_w, self.output_h)      # reshape to conv feature map\n",
        "      x = self.decoder_conv(x)\n",
        "      x = x[:, :, :self.imh, :self.imw]          # crop to original size if needed\n",
        "      return x\n",
        "\n",
        "class VisualAutoencoder( nn.Module):\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 8):\n",
        "        super(VisualAutoencoder, self).__init__()\n",
        "        self.encoder = VisualEncoder(latent_dim, output_w, output_h)\n",
        "        self.decoder = VisualDecoder(latent_dim, output_w, output_h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_content, x_context = self.decoder(z) # Decoder now returns two images\n",
        "        return x_content, x_context # Return both images from the decoder\n"
      ],
      "metadata": {
        "id": "gDZyZc2V7laJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The main sequence predictor model\n",
        "\n",
        "class SequencePredictor(nn.Module):\n",
        "    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim,\n",
        "                 gru_hidden_dim):\n",
        "        super(SequencePredictor, self).__init__()\n",
        "\n",
        "        # --- 1. Static Encoders ---\n",
        "        # (These process one pair at a time)\n",
        "        self.image_encoder = visual_autoencoder.encoder\n",
        "        self.text_encoder = text_autoencoder.encoder\n",
        "\n",
        "        # --- 2. Temporal Encoder ---\n",
        "        # (This processes the sequence of pairs)\n",
        "        fusion_dim = latent_dim * 2 # z_visual + z_text\n",
        "        self.temporal_rnn = nn.GRU(fusion_dim, latent_dim, batch_first=True)\n",
        "\n",
        "        # --- 3. Attention ---\n",
        "        self.attention = CrossModalAttention(gru_hidden_dim)\n",
        "\n",
        "        # --- 4. Final Projection ---\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 3, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # --- 5. Decoders ---\n",
        "        # (These predict the *next* item)\n",
        "        self.image_decoder = visual_autoencoder.decoder\n",
        "        self.text_decoder = text_autoencoder.decoder\n",
        "\n",
        "        # Ensure consistent hidden_dim for decoder's initial states\n",
        "        # This should match the hidden_dim of the DecoderLSTM\n",
        "        self.fused_to_h0 = nn.Linear(latent_dim, self.text_decoder.hidden_dim)\n",
        "        self.fused_to_c0 = nn.Linear(latent_dim, self.text_decoder.hidden_dim)\n",
        "\n",
        "    def forward(self, image_seq, text_seq, target_seq):\n",
        "        # image_seq shape: [batch, seq_len, C, H, W]\n",
        "        # text_seq shape:  [batch, seq_len, text_len]\n",
        "        # target_text_for_teacher_forcing: [batch, text_len] (This is the last text)\n",
        "\n",
        "        batch_size, seq_len, C, H, W = image_seq.shape\n",
        "\n",
        "        # --- 1 & 2: Run Static Encoders over the sequence ---\n",
        "        img_flat = image_seq.view(batch_size * seq_len, C, H, W)\n",
        "        txt_flat = text_seq.view(batch_size * seq_len, -1)\n",
        "\n",
        "        z_v_flat = self.image_encoder(img_flat)\n",
        "        _, hidden_enc, cell_enc = self.text_encoder(txt_flat)\n",
        "\n",
        "        z_fusion_flat = torch.cat((z_v_flat, hidden_enc.squeeze(0)), dim=1)\n",
        "\n",
        "        z_fusion_seq = z_fusion_flat.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # --- 3. Run Temporal Encoder ---\n",
        "        zseq, h = self.temporal_rnn(z_fusion_seq)\n",
        "        # The GRU's 'h' output is (num_layers, batch_size, hidden_size)\n",
        "        # Since num_layers is 1 for our GRU, we can squeeze it.\n",
        "        h = h.squeeze(0) # Shape becomes (batch_size, latent_dim)\n",
        "\n",
        "        # --- 4. Attention ---\n",
        "        # These features should be consistent with the latent_dim (16)\n",
        "        visual_feats_flat = zseq.clone().contiguous().reshape(batch_size * seq_len, latent_dim)\n",
        "        text_feats_expanded = h.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        text_feats_flat = text_feats_expanded.reshape(batch_size * seq_len, latent_dim)\n",
        "\n",
        "        # The CrossModalAttention expects inputs with consistent feature dimensions,\n",
        "        # which should be `latent_dim` (16) as `gru_hidden_dim` was set to `latent_dim`.\n",
        "        context_flat = self.attention(text_feats_flat, visual_feats_flat)\n",
        "        # The output of CrossModalAttention is concatenated img_context and txt_context,\n",
        "        # where each context is `latent_dim`. So the output is `2 * latent_dim`.\n",
        "        context = context_flat.reshape(batch_size, seq_len, latent_dim * 2).mean(dim=1)\n",
        "\n",
        "        # --- 5. Final Prediction Vector (z) ---\n",
        "        z = self.projection(torch.cat((h, context), dim=1))\n",
        "\n",
        "        # --- 6. Decode (Predict pk) ---\n",
        "        pred_image_content, pred_image_context = self.image_decoder(z)\n",
        "\n",
        "        # Generate initial hidden and cell states for the decoder\n",
        "        h0 = self.fused_to_h0(z).unsqueeze(0)\n",
        "        c0 = self.fused_to_c0(z).unsqueeze(0)\n",
        "\n",
        "        # Correcting the decoder_input for the text_decoder\n",
        "        # target_seq (text_target from dataloader) shape is (batch_size, text_len).\n",
        "        # For teacher forcing, we want to feed tokens from index 0 to text_len-2\n",
        "        # to predict tokens from index 1 to text_len-1.\n",
        "        decoder_input = target_seq[:, :-1]\n",
        "\n",
        "        predicted_text_logits_k, _, _ = self.text_decoder(decoder_input, h0, c0)\n",
        "\n",
        "        return pred_image_content, pred_image_context, predicted_text_logits_k, h0, c0\n",
        "\n",
        "# Encoders\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(F.relu(self.fc(x)))\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(F.relu(self.fc(x)))\n",
        "# CrossModalAttention\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Image attends to text and text attends to image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "        self.norm = nn.LayerNorm(hidden_dim*2)\n",
        "\n",
        "    def forward(self, img_feat, txt_feat):\n",
        "        # (B, T, H)\n",
        "        score_img_txt = torch.matmul(\n",
        "            img_feat, txt_feat.transpose(-2, -1)\n",
        "        ) / self.scale\n",
        "\n",
        "        score_txt_img = torch.matmul(\n",
        "            txt_feat, img_feat.transpose(-2, -1)\n",
        "        ) / self.scale\n",
        "\n",
        "        w_img_txt = F.softmax(score_img_txt, dim=-1)\n",
        "        w_txt_img = F.softmax(score_txt_img, dim=-1)\n",
        "\n",
        "        img_context = torch.matmul(w_img_txt, txt_feat)\n",
        "        txt_context = torch.matmul(w_txt_img, img_feat)\n",
        "\n",
        "        fused = torch.cat([img_context, txt_context], dim=-1)\n",
        "        return self.norm(fused)\n",
        "# GRU Sequence modal\n",
        "class SequenceModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return out[:, -1, :] # <--- Modified to return the last output\n",
        "# Baseline Multimodal\n",
        "class BaselineMultimodalModel(nn.Module):\n",
        "     def __init__(self,\n",
        "                 image_dim=128,\n",
        "                 text_dim=64,\n",
        "                 hidden_dim=256,\n",
        "                 seq_hidden=256,\n",
        "                 output_dim=192):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_encoder = ImageEncoder(image_dim, hidden_dim)\n",
        "        self.text_encoder = TextEncoder(text_dim, hidden_dim)\n",
        "        # Use the modified SequenceModel which returns the last output\n",
        "        self.sequence_model = SequenceModel(hidden_dim * 2, seq_hidden)\n",
        "        self.output_layer = nn.Linear(seq_hidden, output_dim)\n",
        "\n",
        "     def forward(self, images, texts):\n",
        "        img_feat = self.image_encoder(images)\n",
        "        txt_feat = self.text_encoder(texts)\n",
        "        fused = torch.cat([img_feat, txt_feat], dim=-1)\n",
        "        seq_out = self.sequence_model(fused)\n",
        "        return self.output_layer(seq_out)\n",
        "# Cross modal attention\n",
        "class AttentionMultimodalModel(nn.Module):\n",
        "      def __init__(self,\n",
        "                 image_dim=128,\n",
        "                 text_dim=64,\n",
        "                 hidden_dim=128,\n",
        "                 seq_hidden=128,\n",
        "                 output_dim=192):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_encoder = ImageEncoder(image_dim, hidden_dim)\n",
        "        self.text_encoder = TextEncoder(text_dim, hidden_dim)\n",
        "        self.cross_attention = CrossModalAttention(hidden_dim)\n",
        "        # Use the modified SequenceModel which returns the last output\n",
        "        self.sequence_model = SequenceModel(hidden_dim * 2, seq_hidden)\n",
        "        self.output_layer = nn.Linear(seq_hidden, output_dim)\n",
        "\n",
        "\n",
        "      def forward(self, images, texts):\n",
        "        img_feat = self.image_encoder(images)\n",
        "        txt_feat = self.text_encoder(texts)\n",
        "        fused = self.cross_attention(img_feat, txt_feat)\n",
        "        seq_out = self.sequence_model(fused)\n",
        "        return self.output_layer(seq_out)\n",
        "\n",
        "# train model\n",
        "def train_model(model, dataloader, epochs):\n",
        "   model.to(device)\n",
        "   optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "   criterion = nn.MSELoss()\n",
        "   losses = []\n",
        "\n",
        "   for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, texts, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, texts)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "   print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "   return losses, avg_loss\n",
        "#Train Baseline (shorter training)\n",
        "\n",
        "print(\"\\nTraining Baseline Model (No Attention)\")\n",
        "baseline_model = BaselineMultimodalModel()\n",
        "baseline_losses, baseline_final = train_model(\n",
        "    baseline_model, dataloader, epochs=10\n",
        ")\n",
        "#  Train Attention Model (longer training)\n",
        "print(\"\\nTraining Cross-Modal Attention Model\")\n",
        "attention_model = AttentionMultimodalModel()\n",
        "attention_losses, attention_final = train_model(\n",
        "    attention_model, dataloader, epochs=20\n",
        ")\n",
        "# Plot Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(baseline_losses, label=\"Baseline (No Attention)\")\n",
        "plt.plot(attention_losses, label=\"Cross-Modal Attention\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Baseline vs Cross-Modal Attention\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Final\n",
        "print(\"\\nFinal Evaluation Results\")\n",
        "print(\"Baseline Final MSE:\", baseline_final)\n",
        "print(\"Cross-Modal Attention Final MSE:\", attention_final)"
      ],
      "metadata": {
        "id": "WCNVm9Nw75pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training utility functions: To initialize and to visualize the progress\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# Plots four images and their reconstructions\n",
        "def validation( model, data_loader ):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    frames, descriptions, image_target, text_target = next(iter(data_loader))\n",
        "\n",
        "    descriptions = descriptions.to(device)\n",
        "    frames = frames.to(device)\n",
        "    image_target = image_target.to(device)\n",
        "    text_target = text_target.to(device)\n",
        "\n",
        "    predicted_image_k,context_image, _, hidden, cell = model(frames, descriptions, text_target)\n",
        "\n",
        "    figure, ax = plt.subplots(2, 6, figsize=(20, 5), gridspec_kw={'height_ratios': [2, 1.5]})\n",
        "\n",
        "    for i in range(4):\n",
        "      im = frames[0, i, :, :, :].cpu()\n",
        "      show_image(ax[0,i], im )\n",
        "      ax[0,i].set_aspect('auto')\n",
        "      ax[0,i].axis('off')\n",
        "      wrapped_text = textwrap.fill(tokenizer.decode(descriptions[0, i, :], skip_special_tokens=True), width=40)\n",
        "\n",
        "      ax[1,i].text(\n",
        "            0.5, 0.99,\n",
        "            wrapped_text,\n",
        "            ha='center',\n",
        "            va='top',\n",
        "            fontsize=10,\n",
        "            wrap=True\n",
        "        )\n",
        "\n",
        "      ax[1,i].axis('off') # Hide axes for the text subplot\n",
        "\n",
        "    show_image(ax[0,4], image_target[0].cpu())\n",
        "    ax[0,4].set_title('Target')\n",
        "    ax[0,4].set_aspect('auto')\n",
        "    ax[0,4].axis('off')\n",
        "    text_target = text_target.squeeze(1)\n",
        "\n",
        "    wrapped_text = textwrap.fill(tokenizer.decode(text_target[0], skip_special_tokens=True), width=40)\n",
        "    ax[1,4].text(\n",
        "            0.5, 0.99,\n",
        "            wrapped_text,\n",
        "            ha='center',\n",
        "            va='top',\n",
        "            fontsize=10,\n",
        "            wrap=False)\n",
        "    ax[1,4].axis('off')\n",
        "    output = context_image[0, :, :, :].cpu()\n",
        "    show_image(ax[0,5], output)\n",
        "    ax[0,5].set_title('Predicted')\n",
        "    ax[0,5].set_aspect('auto')\n",
        "    ax[0,5].axis('off')\n",
        "\n",
        "    generated_tokens = generate(model.text_decoder,\n",
        "                                hidden[:,0, :].unsqueeze(1),\n",
        "                                cell[:, 0, :].unsqueeze(1),\n",
        "                                max_len=150,\n",
        "                                sos_token_id=tokenizer.cls_token_id,\n",
        "                                eos_token_id=tokenizer.sep_token_id)\n",
        "\n",
        "    wrapped_text = textwrap.fill(tokenizer.decode(generated_tokens), width=40)\n",
        "\n",
        "    ax[1,5].text(\n",
        "            0.5, 0.99,\n",
        "            wrapped_text,\n",
        "            ha='center',\n",
        "            va='top',\n",
        "            fontsize=10,\n",
        "            wrap=False )\n",
        "    ax[1,5].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "wAXKT1Xw8AXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Variables and initial setup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "N_EPOCHS = 20\n",
        "emb_dim = 16\n",
        "latent_dim = 16\n",
        "num_layers = 1\n",
        "dropout = True"
      ],
      "metadata": {
        "id": "uwPNs6c38FD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initializing the NLP models\n",
        "encoder = EncoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "decoder = DecoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "text_autoencoder = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "text_autoencoder, _, _, _ = load_checkpoint_from_drive(text_autoencoder, None, filename='text_autoencoder.pth')\n",
        "\n",
        "total_params = sum(p.numel() for p in text_autoencoder.parameters())\n",
        "print(f\"Total parameters (Not trainable): {total_params}\")\n",
        "# Deactivating training from this model for efficiency (although not ideal)\n",
        "for param in text_autoencoder.parameters():\n",
        "        param.requires_grad = False\n",
        "text_autoencoder.eval() # Set to evaluation mode as it's not being trained here"
      ],
      "metadata": {
        "id": "3tkp5R2N8J9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initializing visual models\n",
        "visual_autoencoder = VisualAutoencoder(latent_dim=16)\n",
        "visual_autoencoder.apply(init_weights)\n",
        "\n",
        "total_params = sum(p.numel() for p in visual_autoencoder.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in visual autoencoder: {total_params}\")"
      ],
      "metadata": {
        "id": "cYuQC43W8Ny9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize the main architecture\n",
        "# We put all the sizes the same, not ideal as well\n",
        "sequence_predictor = SequencePredictor(visual_autoencoder, text_autoencoder, latent_dim, latent_dim)\n",
        "sequence_predictor.to(device)\n",
        "\n",
        "# # Print number of trainable parameters\n",
        "total_params = sum(p.numel() for p in sequence_predictor.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in the whole model: {total_params}\")\n",
        "\n",
        "# Print model size\n",
        "total_params = sum(p.numel() for p in sequence_predictor.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "NeB3GP_V8SCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training tools\n",
        "criterion_images = nn.L1Loss()\n",
        "criterion_ctx = nn.MSELoss()\n",
        "criterion_text = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
        "optimizer = torch.optim.Adam(sequence_predictor.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "zJdxqV9f8YH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training loop for the sequence predictor\n",
        "# Instantiate the model, define loss and optimizer\n",
        "\n",
        "sequence_predictor.train()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for frames, descriptions, image_target, text_target  in train_dataloader:\n",
        "\n",
        "      # Send images and tokens to the GPU\n",
        "      descriptions = descriptions.to(device)\n",
        "      frames = frames.to(device)\n",
        "      image_target = image_target.to(device)\n",
        "      text_target = text_target.to(device)\n",
        "      # Predictions from our model\n",
        "      pred_image_content, pred_image_context, predicted_text_logits_k, _, _ = sequence_predictor(frames, descriptions, text_target)\n",
        "\n",
        "      # Computing losses\n",
        "      # Loss for image reconstruction\n",
        "      loss_im = criterion_images(pred_image_content, image_target)\n",
        "      # Loss for the average pattern the images contain\n",
        "      mu_global = frames.mean(dim=[0, 1])\n",
        "      mu_global = mu_global.unsqueeze(0).expand_as(pred_image_context)\n",
        "      loss_context = criterion_ctx(pred_image_context, mu_global)\n",
        "      # Loss function for the text prediction\n",
        "      prediction_flat = predicted_text_logits_k.reshape(-1, tokenizer.vocab_size)\n",
        "      target_labels = text_target.squeeze(1)[:, 1:] # Slice to get [8, 119]\n",
        "      target_flat = target_labels.contiguous().view(-1) # Added .contiguous() for robustness\n",
        "      loss_text = criterion_text(prediction_flat, target_flat)\n",
        "      # Combining the losses\n",
        "      loss = loss_im + loss_text + 0.2*loss_context\n",
        "      # Optimizing\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() * frames.size(0)\n",
        "\n",
        "    # checking model performance on validation set\n",
        "    sequence_predictor.eval()\n",
        "    print(\"Validation on training dataset\")\n",
        "    print( \"----------------\")\n",
        "    validation( sequence_predictor, train_dataloader )\n",
        "    print(\"Validation on validation dataset\")\n",
        "    print( \"----------------\")\n",
        "    validation( sequence_predictor, val_dataloader)\n",
        "    sequence_predictor.train()\n",
        "\n",
        "    # scheduler.step()\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f'Epoch [{epoch+1}/{N_EPOCHS}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "      save_checkpoint_to_drive(sequence_predictor, optimizer, epoch, epoch_loss, filename=f\"sequence_predictor.pth\")\n",
        "\n",
        "# Do better plots\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fZ-XW-MM8hZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example text reconstruction task\n",
        "\n",
        "# Don't forget to unfreeze the model!\n",
        "for param in text_autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(text_autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
        "N_EPOCHS = 20\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    text_autoencoder.train()\n",
        "    epoch_loss = 0\n",
        "    for description in text_dataloader:\n",
        "        # Move the \"sentences\" to device\n",
        "        input_ids = tokenizer(description, return_tensors=\"pt\",  padding=True, truncation=True).input_ids\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # zero the grad, then forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = text_autoencoder(input_ids, input_ids)\n",
        "        # compute the loss: compare 3D logits to 2D targets\n",
        "        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {epoch_loss/len(text_dataloader)}; Latest loss {loss.item()}\")\n",
        "    torch.save(text_autoencoder.state_dict(), f\"seq2seq-epoch-{epoch+1}.pth\")\n",
        "\n",
        "# # saving checkpoint to drive\n",
        "save_checkpoint_to_drive(text_autoencoder, optimizer, 3*N_EPOCHS, loss, filename = \"text_autoencoder.pth\")"
      ],
      "metadata": {
        "id": "yyKgHV1u8mX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Image reonstruction task\n",
        "\n",
        "# To-Do: Use previous labs if you want to pretrain your visual encoder"
      ],
      "metadata": {
        "id": "_zeP4xBp8rlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Computing and showing average images\n",
        "N = 1000\n",
        "H, W = 60, 125\n",
        "\n",
        "# Tensors to accumulate sum (for mean) and sum of squares (for variance)\n",
        "avg_images = [torch.zeros((3, H, W)) for _ in range(5)]\n",
        "sum_sq_diff = [torch.zeros((3, H, W)) for _ in range(5)] # Placeholder for variance numerator\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((H, W)), # Ensure images are resized to HxW\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# --- First Pass: Calculate the Sum (for Mean) ---\n",
        "print(\"Starting Pass 1: Calculating Mean...\")\n",
        "\n",
        "for i in range(N):\n",
        "    # Process sequence i\n",
        "    sequence = train_dataset[i][\"images\"]\n",
        "\n",
        "    for j in range(5):\n",
        "        image = transform(sequence[j])\n",
        "        avg_images[j] += image # Sum for mean\n",
        "\n",
        "# Final step for mean\n",
        "for j in range(5):\n",
        "    avg_images[j] /= N\n",
        "\n",
        "print(\"Starting Pass 2: Calculating Variance...\")\n",
        "\n",
        "for i in range(N):\n",
        "    # Process sequence i\n",
        "    sequence = train_dataset[i][\"images\"]\n",
        "\n",
        "    for j in range(5):\n",
        "        image = transform(sequence[j])\n",
        "\n",
        "        # Calculate (Image - Mean)^2\n",
        "        # Note: We detach the mean from the computation graph if it were being trained,\n",
        "        # but here we're just using it as a fixed statistical value.\n",
        "        diff = image - avg_images[j]\n",
        "        sum_sq_diff[j] += diff * diff # Element-wise squaring\n",
        "\n",
        "# --- Final step for Standard Deviation ---\n",
        "std_images = []\n",
        "for j in range(5):\n",
        "    # Variance = Sum of Squared Differences / N\n",
        "    variance = sum_sq_diff[j] / N\n",
        "\n",
        "    # Standard Deviation = sqrt(Variance)\n",
        "    std_dev = torch.sqrt(variance)\n",
        "    std_images.append(std_dev)\n",
        "\n",
        "print(\"Computation Complete. std_images is a list of 5 tensors (3x60x125).\")\n",
        "# You now have the 5 tensors you need for normalization (mean and std).\n",
        "\n",
        "fig, ax = plt.subplots(1,5, figsize=(15,5))\n",
        "for i in range(5):\n",
        "  avg_image = avg_images[i]\n",
        "\n",
        "  # Printing range of avg_image\n",
        "  print(torch.min(avg_image), torch.max(avg_image))\n",
        "\n",
        "  avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
        "  show_image(ax[i], avg_imagen)\n",
        "\n",
        "# Create a matrix of images with the differences between avg_images\n",
        "fig, ax = plt.subplots(5,5, figsize=(15,8))\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    if i == j:\n",
        "      avg_image = avg_images[i]\n",
        "      avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
        "      show_image(ax[i,j], avg_imagen)\n",
        "    else:\n",
        "      diff = avg_images[i] - avg_images[j]\n",
        "      diff = (diff - torch.min(diff))/(torch.max(diff) - torch.min(diff))\n",
        "      show_image(ax[i,j], diff)\n",
        "    ax[i,j].set_xticks([])\n",
        "    ax[i,j].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(\n",
        "    wspace=0, # Set horizontal space to zero\n",
        "    hspace=0  # Set vertical space to zero\n",
        ")\n"
      ],
      "metadata": {
        "id": "d-Z3no1p8wSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}